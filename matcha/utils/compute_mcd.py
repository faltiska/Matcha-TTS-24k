import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="pyworld")

import argparse
from pathlib import Path
import tempfile
import torch
import torchaudio as ta
from pymcd.mcd import Calculate_MCD

"""
This python script compares audio files generated by the model with the originals files used in training.
It's best to use originals from the validation dataset, which the model did not learn.

Generated files: utterance_*_speaker_*.wav
Original files: original_utterance_*_speaker_*.wav

Script mcd_generate.sh generates speech using phrases from the validation set.
Script mcd_validate.sh runs this python file to compare them against the originals.

Interpretation:
  < 5.0 dB  = Excellent (very close to original)
  5-7 dB    = Good (noticeable but acceptable)
  7-10 dB   = Fair (clear acoustic differences)
  > 10 dB   = Poor (significant timbre/quality issues)

Lower MCD = better acoustic match to ground truth.
"""

def trim_silence(audio: torch.Tensor, sr: int, threshold_db: float = -60.0) -> torch.Tensor:
    """Remove leading and trailing silence from audio."""
    threshold_amp = 10 ** (threshold_db / 20.0)
    window_frames = int(0.01 * sr)  # 10ms
    audio_squared = audio ** 2
    
    pad_size = window_frames - (len(audio) % window_frames)
    if pad_size < window_frames:
        audio_squared = torch.nn.functional.pad(audio_squared, (0, pad_size))
    
    audio_squared = audio_squared.reshape(-1, window_frames)
    rms = torch.sqrt(audio_squared.mean(dim=1))
    
    # Find first non-silent window
    start_idx = 0
    for i in range(len(rms)):
        if rms[i] >= threshold_amp:
            start_idx = i
            break
    
    # Find last non-silent window
    end_idx = len(rms)
    for i in range(len(rms) - 1, -1, -1):
        if rms[i] >= threshold_amp:
            end_idx = i + 1
            break
    
    start_sample = start_idx * window_frames
    end_sample = min(end_idx * window_frames, len(audio))
    
    return audio[start_sample:end_sample]


def trim_audio_file(input_path: Path, output_path: Path, threshold_db: float = -60.0):
    """Load audio, trim silence, and save to output path."""
    audio, sr = ta.load(str(input_path))
    
    # Convert to mono for trimming
    if audio.shape[0] > 1:
        audio_mono = audio[0]
    else:
        audio_mono = audio.squeeze(0)
    
    trimmed = trim_silence(audio_mono, sr, threshold_db)
    
    # Save as mono
    ta.save(str(output_path), trimmed.unsqueeze(0), sr)


def main():
    parser = argparse.ArgumentParser(description="Compare generated wav files to original wav files using MCD")
    parser.add_argument("folder", type=str, help="Folder containing generated and original .wav files")
    args = parser.parse_args()
    
    folder = Path(args.folder)
    
    orig_files = sorted(folder.glob("original_*.wav"))
    if not orig_files:
        print(f"Error: No original_*.wav files found in {folder}")
        return
    
    mcd_toolbox = Calculate_MCD(MCD_mode="dtw")
    
    results = []
    
    for orig_file in orig_files:
        gen_filename = orig_file.name.replace("original_", "", 1)
        gen_file = folder / gen_filename
        
        if not gen_file.exists():
            print(f"{orig_file.name:40s} -> SKIPPED (no matching generated file)")
            continue
        
        # Trim silence from both files, in place
        trim_audio_file(gen_file, gen_file)
        trim_audio_file(orig_file, orig_file)
        
        mcd_value = mcd_toolbox.calculate_mcd(str(gen_file), str(orig_file))
        results.append((gen_file.stem, mcd_value))
        print(f"{gen_file.stem:40s} MCD: {mcd_value:6.2f} dB")
    
    print("-" * 70)
    if results:
        avg_mcd = sum(r[1] for r in results) / len(results)
        print(f"{'Average':40s} MCD: {avg_mcd:6.2f} dB\n")
    else:
        print("\nNo matching pairs found\n")
    
if __name__ == "__main__":
    main()
