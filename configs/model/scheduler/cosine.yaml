# I had to reduce LR 3 times in this training.
#   epoch 0 -> 129 (step 0 -> 26600) - LR = 1e-4
#   epoch 130 -> 229 (step 26600 -> 47000) - LR = 5e-5
#   epoch 230 -> 319 (step 47000 -> 65300) - LR = 2.5e-5
#   epoch 319 -> 410 (step 65300 -> 83600) - LR = 1e-5
#   epoch 410 ->  (step 83600 -> ) - LR = 5e-6
# because the losses were getting too jagged and validation loss was spiking up too high.

# Smooth cosine decay from initial LR to eta_min over T_max epochs
# Eliminates jagged losses and validation spikes by gradually reducing LR
  
scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  _partial_: true
  T_max: 400  # Total number of epochs to reach eta_min; LR stays constant after that. 
  eta_min: 5e-6  # Learning rate to reach at T_max

lightning_args:
  interval: epoch
  frequency: 1
