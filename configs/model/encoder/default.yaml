# TextEncoder model size increased to 12.9M params

encoder_type: RoPE Encoder
encoder_params:
  n_feats: ${model.n_feats}
  # I am training with multiple speakers, so the total number of channels is: (n_channels + spk_emb_dim) = 288
  # (n_channels + spk_emb_dim) / n_heads should be a multiple of 8
  n_channels: 192
  
  # Changed from 768, because I want it to be 3x the total number of channels (and a multiple of 8)
  filter_channels: 864
  
  # Changed from 256 for smarter duration predictor to keep up with the number of speakers
  filter_channels_dp: 320
  
  # Changed n_heads from 2, for better prosody. Allows the encoder to produce a much more 
  # accurate prior mu, reducing the "work" the CFM decoder has to do to reach the target mel.
  n_heads: 6
  
  n_layers: 4
  
  # Increased from 3. It controls receptive field in FFN layers.
  # Each position sees itself + 2 neighbors on each side for local phonetic context
  kernel_size: 5
  
  # Increase to 0.15, if you see overfitting 
  p_dropout: 0.15
  
  # Increased from 64 to 96 (defined in matcha.yaml)
  spk_emb_dim: ${model.spk_emb_dim}
  
  prenet: true

duration_predictor_params:
  filter_channels_dp: ${model.encoder.encoder_params.filter_channels_dp}
  # Changed from 3. 
  # It looks at the phoneme plus 2 neighbors on each side (5 phonemes total)
  kernel_size: 5
  # This is the one most prone to overfitting, I think I can train with 0.15 if using 2 layers.  
  # Original was set to same value as the text encoder (0.1).
  p_dropout: 0.15
  # I tried to use 4 layers, but it's too much, the model overfits quickly.
  # I think, no matter how many speakers I have, there are not so many duration rules to learn.
  n_layers: 2