# TextEncoder capacity increased from:
#   filter_channels: 768 -> 1024
#   filter_channels_dp: 256 -> 384
#   n_heads: 2 -> 8
# TextEncoder model size increased to 12.4M params
encoder_type: RoPE Encoder
encoder_params:
  n_feats: ${model.n_feats}
  # I am training with multiple speakers, so the total number of channels is: (n_channels + spk_emb_dim) = 256
  # (n_channels + spk_emb_dim) / n_heads should best be a power of 2
  n_channels: 192
  # Changed from 768, because filter_channels should be 4x the effective number of channels
  filter_channels: 1024
  # Changed from 256 for smarter duration predictor to keep up with the number of speakers
  filter_channels_dp: 384
  # Changed n_heads from 2 to 8, for better prosody. Allows the encoder to produce a much more 
  # accurate prior mu, reducing the "work" the CFM decoder has to do to reach the target mel.
  n_heads: 8
  n_layers: 6
  kernel_size: 3
  p_dropout: 0.1
  spk_emb_dim: ${model.spk_emb_dim}
  prenet: true

duration_predictor_params:
  filter_channels_dp: ${model.encoder.encoder_params.filter_channels_dp}
  kernel_size: 3
  p_dropout: ${model.encoder.encoder_params.p_dropout}

